\documentclass[12pt,a4paper]{report}

\usepackage{amsmath}
\usepackage{bbm}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage[hyphens]{url}
\usepackage[scale=3]{ccicons}  % per le icone creative commons
\usepackage{hyperref}  % per i link nel pdf
\usepackage[rmargin=3.0cm,lmargin=3.0cm]{geometry}
%\usepackage{frontesp}  % prima pagina; il pacchetto frontesp.sty si trova nella stessa cartella del file .tex (deve essere adattato a mano)
\usepackage{setspace}  % per l'interlinea
\usepackage[english]{babel}  % per sillabazione
\usepackage[all]{xy} %diagrammi di funzioni
\usepackage{xspace} %per assicurare la corretta gestione degli spazi finali quando uso e.g. \AC. NB: sarebbe meglio trovare un'altra soluzione...cfr. http://tex.stackexchange.com/questions/15220/no-space-present-after-ensuremath



\theoremstyle{definition}
\newtheorem{teo}{Teorema}[section]  % resetta la numerazione dei teoremi per ogni capitolo
\newtheorem{defn}[teo]{Definizione}  % la numerazione delle definizioni dipende da quella dei teoremi
\newtheorem{es}[teo]{Esempio}  % idem
\newtheorem{oss}[teo]{Osservazione}  % idem
\newtheorem{prop}[teo]{Proposizione}  % idem
\newtheorem{lemma}[teo]{Lemma}  % idem
\newtheorem{corollario}[teo]{Corollario}  % idem

%%% inizio comandi per stile per teoremi: "numero. Titolo" %%%
\newtheoremstyle{num.custom-title}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\normalfont}  % BODYFONT
  {0pt}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {\thmnumber{#2.}\thmnote{ #3}}
  
\theoremstyle{num.custom-title}  
\newtheorem{teo_custom-title}[teo]{} % per usarlo basta \begin{teo_custom-title}[<Titolo teorema>] (usa automaticamente la numerazione di [teo])
%%% fine comandi per stile per teoremi: "numero. Titolo" %%%


\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\ran}{ran}
\DeclareMathOperator{\orb}{orb}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Zdv}{Zdv}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\A}{\mathcal{A}}
\DeclareMathOperator{\B}{\mathcal{B}}
\DeclareMathOperator{\PP}{\mathcal{P}}
\DeclareMathOperator{\RR}{\mathcal{R}}
\DeclareMathOperator{\LL}{\mathcal{L}}
\DeclareMathOperator{\Hrtg}{\text{Hrtg}}
\DeclareMathOperator{\Ord}{\text{Ord}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\M}{\mathfrak{M}}
\DeclareMathOperator{\U}{\mathfrak{U}}
\DeclareMathOperator{\PPP}{\mathbb{P}}
\DeclareMathOperator{\a01}{\{0,1\}^{\star}}
\DeclareMathOperator{\imp}{\Rightarrow}
\DeclareMathOperator{\sm}{\setminus}
\DeclareMathOperator{\sse}{\subseteq}


\newcommand{\AC}{\ensuremath{\mathsf{AC}}\xspace}
\newcommand{\CC}{\ensuremath{\mathsf{CC}}\xspace}
\newcommand{\DC}{\ensuremath{\mathsf{DC}}\xspace}
\newcommand{\ZF}{\ensuremath{\mathsf{ZF}}\xspace}
\newcommand{\ZFC}{\ensuremath{\mathsf{ZFC}}\xspace}
\newcommand{\LS}{\ensuremath{\mathsf{LS}}\xspace}
\newcommand{\AMC}{\ensuremath{\mathsf{AMC}}\xspace}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} %per la prima pagina
\newcommand{\veedot}{\mathbin{\mathaccent\cdot\vee}}

\renewcommand{\phi}{\varphi}
\renewcommand{\S}{\mathcal{S}}
\renewcommand{\P}{\mathbb{P}}
\renewcommand{\1}{\mathbbm{1}}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\Im}{\operatorname{Im}}


%%%% INIZIO COMANDI PER EQUIVALENZE %%%%
\newcommand{\Implies}[2]{$\text{\ref{statement#1}}\!\implies\!\text{\ref{statement#2}}$}% X => Y
\newcommand{\punto}[1]{\item \label{statement#1}}


\newenvironment{equivalence}
    {\begin{enumerate}[label=(\arabic*),ref=(\arabic*)]
    }
    { 
	\end{enumerate}
    }
%%%% FINE COMANDI PER EQUIVALENZE %%%



% Interlinea 1.5
%\onehalfspacing  


%per le citazioni
\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip2em
  \hbox{}\nobreak\hfil(#1)%
  \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

\newsavebox\mybox
\newenvironment{aquote}[1]
  {\savebox\mybox{#1}\begin{quote}}
  {\signed{\usebox\mybox}\end{quote}}

%disabilita colore link
\hypersetup{%
    pdfborder = {0 0 0}
}

\begin{document}

\paragraph{Exercise 16.} Suppose $F$ strictly increasing. This means that $F$ is bijective, i.e. there exists $F^{-1}$ inverse function of $F$. Note also that, given a r.v. $Y$, $F^{-1}(Y)$ is still a r.v., since $F^{-1}$ is still continuous (should be proved formally), hence measurable. Observe now that 
\[
\P[F(X) \leq x] = \P[F^{-1}(F(X)) \leq F^{-1}(x)] = \P[X \leq F^{-1}(x)] = F(F^{-1}(x)) = x.
\]
That is, $F(X) \sim U$, the uniform distribution on $F[\Im X]$.\\
\\
We want to prove that the same holds even if $F$ is not strictly monotone. Suppose then $F$ increasing, but not strictly. That is, there exist intervals where $F$ is constant, and other intervals where $F$ is injective (restricted on those intervals). We may assume that those intervals are maximal, i.e. disjoint (otherwise we can just sustitute them with the union).\\
\textbf{Claim:} such intervals are at most countably many.
\begin{proof}[Proof (euristic).]
Thanks to the disjoint hypothesis, we have that on every interval $F$ increases of some factor $a_\alpha>0$ compared to the previous ones. Let $\{a_\alpha : \alpha \in \A\}$ be the set of increases. Since $\lim_\infty F(x) = 1$, we have that 
\[
\sum_{\alpha \in \A} a_\alpha = 1 < +\infty.
\]
A well known analysis lemma assures us that, since the sum is finite, $\A$ must be at most countable.
\end{proof}
Therefore we can write $F$ as 
\[
F(x)=F_0(x) \1_{A_0}(x) + \sum_{i=1}^\infty c_i \1_{A_i}(x),
\]
where $A_0$ is a set, $F_0(x)$ is strictly increasing (on $A_0$), $c_i$ are constant $\geq 0$, $A_i$ is a closed interval $[a_i,b_i]$ for any $i \geq 1$, and all the $A_i$, $i \in \N$, are pairwise disjoint and their union is the whole domain of $F$.

We can provide a right inverse $F^{-1}$ of $F$, e.g. the one defined by
\[
F^{-1}(x):=F_0^{-1}(x) \1_{F[A_0]}(x) + \sum_{i=1}^\infty a_i \1_{F[A_i]}(x).
\]
It is easy to check that $F^{-1}$ is (strictly) increasing (try with a simple drawing). Furthermore, it is measurable, since it can be written as a limit of (sums of) measurable functions. We have:
\begin{align*}
\P[F(X) \leq x] 
&= \P \left[ F^{-1}(F(X)) \leq F^{-1}(x) \right]\\
&= \P \left[ F_0^{-1}(F(X)) \1_{F[A_0]}(F(X)) + \sum_{i=1}^\infty a_i \1_{F[A_i]}(F(X)) \leq F^{-1}(x) \right]\\
&= \P \left[ X \, \1_{A_0}(X) + \sum_{i=1}^\infty a_i \1_{A_i}(X) \leq F^{-1}(x) \right]\\
&= \P \left[ X \, \1_{A_0}(X) + X \1_{\cup_{i=1}^\infty A_i}(X) \leq F^{-1}(x) \right]\\
&= \P \left[ X \leq F^{-1}(x) \right] = F(F^{-1}(x)) = x, \\
\end{align*}
where the last equality holds because $F^{-1}$ is a right inverse of $F$, and the fourth equality holds thanks to the following observation: $X \, \1_{A_0}(X) + \sum_{i=1}^\infty a_i \1_{A_i}(X)$ and $X \, \1_{A_0}(X) + X \1_{\cup_{i=1}^\infty A_i}(X)$ are equal almost everywhere (thus, our equality holds). In order to prove that, it is sufficient to prove that $\1_{\cup_{i=1}^\infty A_i}(X)$ is null almost everywhere. Observe that, for every $i \geq 1$,
\[
\1_{A_i}(X) \neq 0 \iff X \in A_i \iff a_i \leq X \leq b_i.
\]
Hence $P(X \in A_i) = F(b_i)-F(a_i)$. But by construction of the $A_i$'s, we have $0=F(b_i)-F(a_i)=\P(X \in A_i)$. Thus every $A_i$ has zero measure, and therefore also $\bigcup_{i=1}^\infty A_i$ has zero measure, since it is the countable union of zero-measure sets. $\qed$

\paragraph{Exercise 17.} We already consider the general case where $F$ is increasing, but not necessarily strictly. Let $F^{-1}$ be the right inverse of $F$ defined like in the previous exercise (which is measurable, as explained above). We want $\P[G(U) \leq x] = F(x)$, thus $\P[G(U) \leq F^{-1}(x)] = F(F^{-1}(x))$, thus $\P[F(G(U)) \leq F(F^{-1}(x))] = x$, thus $\P[F \circ G (U) \leq x] = x$, thus if $F \circ G = \id$ it works, i.e. $G=F^{-1}$. It is straightforward to check that defining $G:=F^{-1}$ we are done. \qed

\paragraph{Exercise 18.} We want to find $\lambda(\{\omega \in [0,1) \mid \lfloor 2^n \omega \rfloor \text{ is even}\})$. Observe that, given $\omega \in \R$, $\lfloor 2^n \omega \rfloor \text{ is even}$ iff $2^n \omega = 2m + \mu$ for some $m \in \N$ and $\mu \in [0,1)$. Thus $\omega = \frac{2m}{2^n} + \frac{\mu}{2^n}$. Let's now fix $m$. We obviously have that 
\[
\lambda \left( \left\{ \omega \in \R \Big | \omega = \frac{2m}{2^n} + \frac{\mu}{2^n}, \ \mu \in [0,1) \right\} \right) = \frac{1}{2^n}.
\]
Observe now that changing $m$, we obtain a different disjoint set:
\[
\frac{2m}{2^n} + \frac{\mu}{2^n} = \frac{2m'}{2^n} + \frac{\mu'}{2^n} \imp \mu = 2(m'-m) + \mu' \imp m=m' \text{ since } \mu \in [0,1).
\]
Finally, we must find the maximum possible value for $m$ (since $0 \leq \omega \leq 1$): 
\[
\frac{2m+\mu}{2^n} < 1 \imp m < \frac{2^n-\mu}{2} = 2^{n-1}-\mu/2 \imp m \leq 2^{n-1}-1.
\]
So we obtain 
\begin{multline*}
\lambda(\{\omega \in [0,1) \mid \lfloor 2^n \omega \rfloor \text{ is even}\}) = \lambda \left( \biguplus_{m=0}^{2^{n-1}-1} \left\{ \omega \in \R \Big | \omega = \frac{2m}{2^n} + \frac{\mu}{2^n}, \ \mu \in [0,1) \right\} \right)\\
= \sum_{m=0}^{2^{n-1}-1} \lambda \left( \left\{ \omega \in \R \Big | \omega = \frac{2m}{2^n} + \frac{\mu}{2^n}, \ \mu \in [0,1) \right\} \right) = \sum_{m=0}^{2^{n-1}-1} \frac{1}{2^n} = 2^{n-1} \cdot \frac{1}{2^n} = \frac{1}{2}.
\end{multline*}
So $\P[X_n = 0] = \frac{1}{2}$, and of course $\P[X_n = 1] = 1-\frac{1}{2} = \frac{1}{2}$. Thus $X_n$ is distributed like a bernullian $B(1,\frac{1}{2})$, for every $n$.\\
We now want to prove that the $X_n$'s are independent. First, observe that for all $\omega \in (0,1]$, $\omega$ can be written in binary as $0.x_1x_2x_3x_4...$, where $x_n=0,1$. Observe that this means 
\[
\omega = \sum_{n=1}^\infty 2^{-n} x_n.
\]
Now consider $\lfloor 2^n\omega \rfloor = 2^{n-1} x_1 + 2^{n-2} x_2 + ... + 2 x_{n-1} + x_n$. It is thus clear that $\lfloor 2^n\omega \rfloor$ is even iff $x_n=0$. I.e., $X_n(\omega) = x_n$.\\
So consider $\P[X_1=i_1, X_2=i_2,...,X_n=i_n]$ for any arbitrary $n \in \N$. For what we just said, we are calculating the probability that the first $n$ digits of the binary representation of $\omega$ are $i_1,...,i_n$. That is, we want to calculate the Lebesgue measure of the set of the $\omega$'s such that $\omega \in [0.i_1i_2...i_n0000...; \, 0.i_1i_2...i_n1111...]$, which by invariance under translations equals the Lebesgue measure of $[0; \, 0.000..00011111....]$. The Lebesgue measure of that interval is of course
\[
\sum_{i=n}^\infty 2^{-i}.
\]
Since this holds for any $n \in \N$, this means that $\P[X_1=i_1, X_2=i_2,...,X_n=i_n] = \sum_{i=n}^\infty 2^{-i} = \frac{1}{2} \cdot \sum_{i=n}^\infty 2^{-i+1} = \frac{1}{2} \cdot \sum_{i=n-1}^\infty 2^{-i} = \P[X_n=i_n] \cdot \P[X_1=i_1,...,X_{n-1}=i_{n-1}]$.\\
The last step is to prove independence in the general case. So, consider $\{X_{k_1},...,X_{k_n}\}$. We can suppose WLOG that $k_1<k_2<...<k_n$. Furthermore, in order to lighten the notation, we suppose that $n>2$, $k_1=2$, $k_2=4$ and $k_{i+1}=k_i+1$ for all $i \geq 2$. Observe that, thanks to the previous less general case, we have
\begin{multline*}
\P[X_{k_1}=i_1,...,X_{k_n}=i_n] = \P[X_1 \in \R, X_{k_1}=i_1, X_2 \in \R, X_{k_2}=i_2,...,X_{k_n}=i_n] \\
=  \P[X_1=0, X_{k_1}=i_1, X_3=0, X_{k_2}=i_2,...,X_{k_n}=i_n] \\
+ \P[X_1=1, X_{k_1}=i_1, X_3=0, X_{k_2}=i_2,...,X_{k_n}=i_n] \\
+ \P[X_1=0, X_{k_1}=i_1, X_3=1, X_{k_2}=i_2,...,X_{k_n}=i_n] \\
+ \P[X_1=1, X_{k_1}=i_1, X_3=1, X_{k_2}=i_2,...,X_{k_n}=i_n] \\
= \text{... we split into products thanks to previous point ...}\\
= \P[X_{k_1}=i_1] \cdot ... \cdot \P[X_{k_n}=i_n] \big( \P[X_1=0] \P[X_3=0] + \P[X_1=1] \P[X_3=0] \\
+ \P[X_1=0] \P[X_3=1] + \P[X_1=1] \P[X_3=1] \big) \\
= \P[X_{k_1}=i_1] \cdot ... \cdot \P[X_{k_n}=i_n] \left( \frac{1}{4}+\frac{1}{4}+\frac{1}{4}+\frac{1}{4} \right) \\
= \P[X_{k_1}=i_1] \cdot ... \cdot \P[X_{k_n}=i_n],
\end{multline*}
and we are done (since the most general argument proceeds by induction, for example on the number of ``holes''). \qed

\paragraph{Exercise 19.} Of course $0 \leq \sum_{n=1}^\infty 2^{-n} X_n \leq 1$. We want to calculate 
\[
\P \left[ \sum_{n=1}^\infty 2^{-n} X_n \leq x \right].
\]
We can write $x$ in base 2 as $x= \sum_{n=1}^\infty 2^{-n} x_n$ with $x_n=0,1$. Observe now that
\[
\sum_{n=1}^\infty 2^{-n} X_n \leq \sum_{n=1}^\infty 2^{-n} x_n 
\]
\centerline{if and only if }
\[
(X_1=x_1 \wedge X_2=x_2 \wedge ... ) \veedot (X_1 < x_1) \veedot (X_1=x_1 \wedge X_2 < x_2) \veedot (X_1=x_1 \wedge X_2=x_2 \wedge X_3 < x_3) \veedot ... 
\]
Observe now that $\P[X_i<x_i] \neq 0 \iff x_i=1 \wedge X_i=0$. Since $\P(X_i=0)=\P(X_i=1)=\frac{1}{2}$, we have
\begin{align*}
\P \left[ \sum_{n=1}^\infty 2^{-n} X_n \leq x \right]
&= \P[X_1=x_1 \wedge X_2=x_2 \wedge ...] + \sum_{i=1}^\infty \P \left[ X_i<x_i \wedge \bigwedge_{j=1}^{i-1} (X_j=x_j) \right]\\
&= 0 + \sum_{i=1}^\infty \left( \P[X_i<x_i] \prod_{j=1}^{i-1} \P[X_j=x_j] \right)\\
&= \sum_{i=1}^\infty \P[X_i<x_i] \cdot \frac{1}{2^{i-1}}\\
&= \sum_{\{i \mid x_i=1\}} \P[X_i<x_i] \cdot \frac{1}{2^{i-1}}\\ 
&= \sum_{\{i \mid x_i=1\}} \P[X_i=0] \cdot \frac{1}{2^{i-1}} = \sum_{\{i \mid x_i=1\}} \frac{1}{2} \cdot \frac{1}{2^{i-1}} = \sum_{\{i \mid x_i=1\}} \frac{1}{2^i} = x,
\end{align*}
where the second equality holds because of the independence of the $X_i$'s.\\
Therefore the distribution $\sum_{n=1}^\infty 2^{-n} X_n$ is distributed like the uniform distribution on $[0,1]$. \qed

\paragraph{Exercise 20.} Take the sequence $(X_n)_{n \geq 1}$ of Bernoulli r.v. of Exercise 18. Let $\phi: \N_{\geq 1} \to \N_{\geq 1} \times \N_{\geq 1}$ be a bijection (for example the standard diagonal enumeration). We can extract countably many \emph{disjoint} sequences of r.v. this way: for any $i \geq 1$, define
\[
(X_{i,j})_{j \geq 1} := (X_{\phi(i,j)})_{j \geq 1}.
\]
By the previous exercise, it is clear that, for every $i \geq 1$, $U_i:= \sum_{j=1}^\infty 2^{-j} X_{i,j}$ is a uniform distribution on $[0,1]$.\\
We want to prove that the $U_i$'s are independent. In order to lighten the notation, we will consider just two random variables (the general case being similar) and we will write them as $U=\sum_{i=1}^\infty 2^{-i} X_i$ and $V=\sum_{i=1}^\infty 2^{-i} Y_i$. Observe that:
\begin{align*}
\P[U \leq t, V \leq z]
&= \P \left[ \sum_{n=1}^\infty 2^{-n} X_n \leq \sum_{n=1}^\infty 2^{-n} t_n \ \wedge \ \sum_{n=1}^\infty 2^{-n} Y_n \leq \sum_{n=1}^\infty 2^{-n} z_n \right]\\
&= \P \big[ \big( (X_1=t_1 \wedge X_2=t_2 \wedge ... ) \veedot (X_1 < t_1) \veedot (X_1=t_1 \wedge X_2 < t_2) \veedot ... \big) \\
& \wedge \big( (Y_1=z_1 \wedge Y_2=z_2 \wedge ... ) \veedot (Y_1 < z_1) \veedot (Y_1=z_1 \wedge Y_2 < z_2) \veedot ... \big)  \big]\\
&= \P[ \text{ ... ``convolution product'' of logical connectives ... }]\\
&= \text{ ... convolution product of the two sums (thanks to independence) ... }\\
&= \left( \sum_{\{i \mid t_i=1\}} \frac{1}{2^i} \right) \cdot \left( \sum_{\{i \mid z_i=1\}} \frac{1}{2^i} \right) = t \cdot z = \P[U \leq t] \cdot \P[V \leq z],
\end{align*}
and we are done. \qed

\paragraph{Exercise 21.} Take the sequence $(U_n)_{n \geq 1}$ of independent uniform r.v. of the previous exercise. Define $X_n:=F^{-1}(U_n)$ like in our solution of Exercise 17. Thanks to Exercise 17, we know that every $X_n$'s cumulative distribution is $F(x)$.\\
We shall prove that the $X_n$'s are independent. Observe that
\begin{align*}
\P[X_n \leq t, X_m \leq z]
&= \P[F^{-1}(U_i) \leq t, F^{-1}(U_j) \leq z]\\
&= \P[F(F^{-1}(U_i)) \leq F(t), F(F^{-1}(U_j)) \leq F(z)]\\
&= \P[U_i \leq F(t), U_j \leq F(z)]\\
&= \P[U_i \leq F(t)] \cdot \P[U_j \leq F(z)]\\
&= F(t) \cdot F(z) = \P[X_n \leq t] \cdot \P[X_m \leq z],
\end{align*}
where the third equality holds because $F^{-1}$ is a right inverse of $F$ and the fourth equality holds thanks to the independence of the $U_n$'s. \qed



\end{document}